{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b7c096",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172f9594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    user_id             song_id  plays\n",
      "0  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1\n",
      "1  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1\n",
      "2  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2\n",
      "3  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1\n",
      "4  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åŠ è½½æ’­æ”¾è®°å½•ï¼ˆç”¨æˆ·-æ­Œæ›²-æ’­æ”¾æ¬¡æ•°ï¼‰\n",
    "triplets = pd.read_csv('train_triplets.txt', sep='\\t', header=None, names=['user_id', 'song_id', 'plays'])\n",
    "print(triplets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0f2b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ‡é¢˜ç¼ºå¤±æ¯”ä¾‹: 0.0\n",
      "ç¤ºä¾‹æ•°æ®:\n",
      "                                    user_id             song_id  plays  \\\n",
      "0  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1   \n",
      "1  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1   \n",
      "2  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2   \n",
      "3  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1   \n",
      "4  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1   \n",
      "\n",
      "                             title  \n",
      "0                         The Cove  \n",
      "1             Nothing from Nothing  \n",
      "2                  Entre Dos Aguas  \n",
      "3            Under Cold Blue Stars  \n",
      "4  Riot Radio (Soundtrack Version)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def load_metadata(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        songs_dataset = f['metadata']['songs']\n",
    "        \n",
    "        # æå–åŸå§‹å­—èŠ‚æ•°æ®\n",
    "        song_ids_bytes = songs_dataset['song_id'][()]  # å­—èŠ‚æ•°ç»„\n",
    "        titles_bytes = songs_dataset['title'][()]      # å­—èŠ‚æ•°ç»„\n",
    "        \n",
    "        # å®‰å…¨è§£ç ä¸º UTF-8ï¼ˆå¤„ç†éæ³•å­—ç¬¦ï¼‰\n",
    "        song_ids = [s.decode('utf-8', errors='ignore').strip() for s in song_ids_bytes]\n",
    "        titles = [t.decode('utf-8', errors='ignore').strip() for t in titles_bytes]\n",
    "        \n",
    "        # æ„å»º DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'song_id': song_ids,\n",
    "            'title': titles\n",
    "        })\n",
    "        \n",
    "        # ç§»é™¤ç©º song_id\n",
    "        df = df[df['song_id'].str.len() > 0]\n",
    "    return df\n",
    "\n",
    "metadata = load_metadata('msd_summary_file.h5')\n",
    "\n",
    "# åˆå¹¶æ•°æ®\n",
    "merged_data = pd.merge(\n",
    "    triplets,\n",
    "    metadata,\n",
    "    on='song_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# éªŒè¯ç»“æœ\n",
    "print(\"æ ‡é¢˜ç¼ºå¤±æ¯”ä¾‹:\", merged_data['title'].isnull().mean())\n",
    "print(\"ç¤ºä¾‹æ•°æ®:\")\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3847b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data.head())\n",
    "# æ˜¾ç¤ºå‰10ä¸ªç”¨æˆ·å¬è¿‡çš„æ­Œæ›²(æŒ‰æ’­æ”¾æ¬¡æ•°æ’åº)\n",
    "top10_users = merged_data['user_id'].value_counts().head(10).index.tolist()\n",
    "for uid in top_users:\n",
    "    print(f\"ç”¨æˆ· {uid} å¬è¿‡çš„æ­Œæ›²ï¼ˆæŒ‰æ’­æ”¾æ¬¡æ•°æ’åºï¼‰:\")\n",
    "    user_songs = merged_data[merged_data['user_id'] == uid]\n",
    "    user_songs_sorted = user_songs.sort_values(by='plays', ascending=False)\n",
    "    for title in user_songs_sorted['title']:\n",
    "        print(f\"  - {title}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094746",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3feedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ†• åˆ›å»ºæ–°çš„MLPæ¨¡å‹\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embed (Embedding)          (None, 1, 64)        65236352    user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "item_embed (Embedding)          (None, 1, 64)        24610944    item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 64)           0           user_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 64)           0           item_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          33024       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 64)           8256        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            65          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 89,921,537\n",
      "Trainable params: 89,921,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5256\\1599165615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;31m# è®­ç»ƒæ¨¡å‹ï¼ˆepochså¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m history = model.fit(\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# è®¾ç½®è¾ƒå¤§å€¼ï¼Œä¾é æ—©åœæœºåˆ¶\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Multiply, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Flatten, \n",
    "    Concatenate, Dropout, Dense  \n",
    ")\n",
    "# ----------------------\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "# ----------------------\n",
    "\n",
    "# 1. ç”¨æˆ·å’Œæ­Œæ›²IDç¼–ç ï¼ˆè½¬æ¢ä¸ºè¿ç»­æ•´æ•°ï¼‰\n",
    "user_encoder = LabelEncoder()\n",
    "song_encoder = LabelEncoder()\n",
    "\n",
    "# å¯¹ç”¨æˆ·IDå’Œæ­Œæ›²IDè¿›è¡Œç¼–ç \n",
    "merged_data['user_id_encoded'] = user_encoder.fit_transform(merged_data['user_id'])\n",
    "merged_data['song_id_encoded'] = song_encoder.fit_transform(merged_data['song_id'])\n",
    "\n",
    "# 2. å½’ä¸€åŒ–æ’­æ”¾æ¬¡æ•°åˆ° [0,1]\n",
    "max_play = merged_data['plays'].max()\n",
    "merged_data['plays_normalized'] = merged_data['plays'] / max_play\n",
    "\n",
    "# 3. æå–è®­ç»ƒæ•°æ®\n",
    "user_ids = merged_data['user_id_encoded'].values\n",
    "item_ids = merged_data['song_id_encoded'].values\n",
    "labels = merged_data['plays_normalized'].values  # å½’ä¸€åŒ–åçš„æ’­æ”¾æ¬¡æ•°\n",
    "\n",
    "# 4. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_user, test_user, train_item, test_item, train_label, test_label = train_test_split(\n",
    "    user_ids, item_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# æ¨¡å‹æ„å»º\n",
    "# ----------------------\n",
    "\n",
    "# å®šä¹‰ç”¨æˆ·å’Œç‰©å“æ•°é‡\n",
    "num_users = len(user_encoder.classes_)  # 105,283\n",
    "num_items = len(song_encoder.classes_)  # 384,546\n",
    "embedding_size = 32  # åµŒå…¥ç»´åº¦\n",
    "\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "def build_mlp_model():\n",
    "    pretrained_path = \"best_mlp_model.keras\"\n",
    "    \n",
    "    if Path(pretrained_path).exists():\n",
    "        print(\"â³ æ£€æµ‹åˆ°é¢„è®­ç»ƒMLPæ¨¡å‹ï¼ŒåŠ è½½ä¸­...\")\n",
    "        model = tf.keras.models.load_model(pretrained_path)\n",
    "        print(\"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹\")\n",
    "        \n",
    "        # è°ƒæ•´å­¦ä¹ ç‡\n",
    "        new_learning_rate = 0.0001\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=new_learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    else:\n",
    "        print(\"ğŸ†• åˆ›å»ºæ–°çš„MLPæ¨¡å‹\")\n",
    "        # è¾“å…¥å±‚\n",
    "        user_input = Input(shape=(1,), dtype=tf.int32, name='user_input')\n",
    "        item_input = Input(shape=(1,), dtype=tf.int32, name='item_input')\n",
    "        \n",
    "        # åµŒå…¥å±‚ï¼ˆé€‚å½“å¢åŠ ç»´åº¦ï¼‰\n",
    "        user_embed = Embedding(num_users, 64, name='user_embed')(user_input)\n",
    "        item_embed = Embedding(num_items, 64, name='item_embed')(item_input)\n",
    "        \n",
    "        # å±•å¹³åµŒå…¥å‘é‡\n",
    "        user_flatten = Flatten()(user_embed)\n",
    "        item_flatten = Flatten()(item_embed)\n",
    "        \n",
    "        # æ‹¼æ¥ç‰¹å¾\n",
    "        concat = Concatenate()([user_flatten, item_flatten])\n",
    "        \n",
    "        # æ·±åº¦å…¨è¿æ¥å±‚\n",
    "        dense = Dense(256, activation='relu')(concat)\n",
    "        dense = Dropout(0.2)(dense)  # æ·»åŠ Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "        dense = Dense(128, activation='relu')(dense)\n",
    "        dense = Dense(64, activation='relu')(dense)\n",
    "        \n",
    "        # è¾“å‡ºå±‚ï¼ˆå›å½’ä»»åŠ¡ä½¿ç”¨çº¿æ€§æ¿€æ´»ï¼‰\n",
    "        output = Dense(1, activation='linear')(dense)\n",
    "        \n",
    "        model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "# ----------------------\n",
    "# ä¿®æ”¹è®­ç»ƒæµç¨‹ï¼ˆæ›¿æ¢æ¨¡å‹æ„å»ºéƒ¨åˆ†ï¼‰\n",
    "# ----------------------\n",
    "model = build_mlp_model()\n",
    "model.summary()\n",
    "# åˆ›å»ºæ—©åœå›è°ƒ\n",
    "# è½¬æ¢ä¸ºTensorFlow Datasetï¼ˆæå‡æ€§èƒ½ï¼‰\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_input\": train_user, \"item_input\": train_item}, train_label)\n",
    ").shuffle(100000, reshuffle_each_iteration=True).batch(16384).cache().prefetch(tf.data.AUTOTUNE)  # è‡ªåŠ¨é¢„åŠ è½½\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"user_input\": test_user, \"item_input\": test_item}, test_label)\n",
    ").batch(16384).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# æ·»åŠ æ¨¡å‹ä¿å­˜å›è°ƒï¼ˆè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰\n",
    "class CustomCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # æ·»åŠ ä¼˜åŒ–å™¨çŠ¶æ€ä¿å­˜è·¯å¾„\n",
    "        self.optimizer_path = \"optimizer_state_mlp.pkl\"\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        # ä¿å­˜ä¼˜åŒ–å™¨æƒé‡\n",
    "        joblib.dump(self.model.optimizer.get_weights(), self.optimizer_path)\n",
    "        print(f\"ğŸ’¾ å·²ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€è‡³ {self.optimizer_path}\")\n",
    "\n",
    "# æ›´æ–°æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "checkpoint = CustomCheckpoint(\n",
    "    \"best_mlp_model.keras\",  # ä¿®æ”¹ä¿å­˜æ–‡ä»¶å\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# å¦‚æœæ£€æµ‹åˆ°ä¼˜åŒ–å™¨çŠ¶æ€åˆ™åŠ è½½\n",
    "if Path(\"optimizer_state_mlp.pkl\").exists():\n",
    "    print(\"â³ åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€...\")\n",
    "    optimizer_weights = joblib.load(\"optimizer_state_mlp.pkl\")\n",
    "    model.optimizer.set_weights(optimizer_weights)\n",
    "    print(\"âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤\")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹ï¼ˆepochså¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=10,  # è®¾ç½®è¾ƒå¤§å€¼ï¼Œä¾é æ—©åœæœºåˆ¶\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# æ¨¡å‹è¯„ä¼°ä¸é¢„æµ‹\n",
    "# ----------------------\n",
    "\n",
    "# è¯„ä¼°æµ‹è¯•é›†\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f\"testsets MSE: {test_loss:.4f}, MAE: {test_mae:.4f}\")\n",
    "\n",
    "# ä¿å­˜ç¼–ç å™¨ï¼ˆè®­ç»ƒåç«‹å³æ‰§è¡Œï¼‰\n",
    "joblib.dump(max_play, 'max_play_mlp.pkl')\n",
    "joblib.dump(user_encoder, 'user_encoder_mlp.pkl')\n",
    "joblib.dump(song_encoder, 'song_encoder_mlp.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import h5py\n",
    "# ----------------------\n",
    "# Load metadata \n",
    "# ----------------------\n",
    "def load_metadata(filename):\n",
    "    \"\"\"Metadata loading function identical to the training code\"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        songs_dataset = f['metadata/songs']  # Note the hierarchy uses '/' separators\n",
    "        \n",
    "        # Extract all required fields\n",
    "        song_ids_bytes = songs_dataset['song_id'][()]\n",
    "        titles_bytes = songs_dataset['title'][()]\n",
    "        artists_bytes = songs_dataset['artist_name'][()]  # Added artist field\n",
    "        \n",
    "        # Unified decoding process\n",
    "        decode_func = lambda x: x.decode('utf-8', errors='ignore').strip()\n",
    "        song_ids = list(map(decode_func, song_ids_bytes))\n",
    "        titles = list(map(decode_func, titles_bytes))\n",
    "        artists = list(map(decode_func, artists_bytes))\n",
    "        \n",
    "        # Build DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'song_id': song_ids,\n",
    "            'title': titles,\n",
    "            'artist_name': artists  # Add other required fields\n",
    "        })\n",
    "        \n",
    "        # Filter invalid data\n",
    "        return df[df['song_id'].str.len() > 0]\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "\n",
    "# Enhanced Recommendation System Class\n",
    "# ----------------------\n",
    "class AdvancedMusicRecommender:\n",
    "    def __init__(self):\n",
    "        # Load metadata\n",
    "        self.metadata = load_metadata('msd_summary_file.h5')\n",
    "        \n",
    "        # Load model and encoders\n",
    "        self.model = tf.keras.models.load_model('best_mlp_model.keras')\n",
    "        self.song_encoder = joblib.load('song_encoder_mlp.pkl')\n",
    "        \n",
    "        # Handle missing max_play\n",
    "        try:\n",
    "            self.max_play = joblib.load('max_play_mlp.pkl')\n",
    "        except FileNotFoundError:\n",
    "            print(\"Warning: Estimating max_play using metadata\")\n",
    "            self.max_play = self.metadata['plays'].max() if 'plays' in self.metadata else 1\n",
    "        \n",
    "        # Create song ID to index mapping\n",
    "        self.song_id_to_idx = {\n",
    "            song_id: idx \n",
    "            for idx, song_id in enumerate(self.song_encoder.classes_)\n",
    "        }\n",
    "        \n",
    "        # Get song embeddings\n",
    "        self.song_embeddings = self.model.get_layer('item_embed').get_weights()[0]\n",
    "\n",
    "    def search_songs(self, query, top_k=5):\n",
    "        \"\"\"Modified search function with consistent fields\"\"\"\n",
    "        mask = (\n",
    "            self.metadata['title'].str.contains(query, case=False) |\n",
    "            self.metadata['artist_name'].str.contains(query, case=False)\n",
    "        )\n",
    "        return self.metadata[mask].head(top_k)[['song_id', 'title', 'artist_name']]\n",
    "    \n",
    "    def create_virtual_user(self, song_ids):\n",
    "        \"\"\"Create virtual user features from song IDs\"\"\"\n",
    "        valid_ids = [song_id for song_id in song_ids if song_id in self.song_id_to_idx]\n",
    "        \n",
    "        if not valid_ids:\n",
    "            raise ValueError(\"No valid song IDs found\")\n",
    "            \n",
    "        indices = [self.song_id_to_idx[song_id] for song_id in valid_ids]\n",
    "        avg_embedding = np.mean(self.song_embeddings[indices], axis=0)\n",
    "        return avg_embedding\n",
    "\n",
    "    def _select_songs_interactively(self, matched_songs):\n",
    "        \"\"\"Interactive song selection with re-search option\"\"\"\n",
    "        print(\"\\nğŸ” Found matching songs:\")\n",
    "        print(\"0. Search again (unsatisfied with results)\")\n",
    "        for idx, (_, row) in enumerate(matched_songs.iterrows(), 1):\n",
    "            print(f\"{idx}. {row['title']} - {row['artist_name']}\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                selected = input(\"Enter song numbers (space-separated, 0 to re-search, enter for all): \").strip()\n",
    "                if not selected:\n",
    "                    return matched_songs['song_id'].tolist()\n",
    "                \n",
    "                if '0' in selected.split():\n",
    "                    return None\n",
    "                \n",
    "                indices = list(map(int, selected.split()))\n",
    "                valid_indices = [i for i in indices if 1 <= i <= len(matched_songs)]\n",
    "                \n",
    "                if not valid_indices:\n",
    "                    print(\"âš ï¸ Invalid input, please try again\")\n",
    "                    continue\n",
    "                \n",
    "                return matched_songs.iloc[[i-1 for i in valid_indices]]['song_id'].tolist()\n",
    "            \n",
    "            except ValueError:\n",
    "                print(\"âš ï¸ Please enter valid numbers\")\n",
    "\n",
    "    def _format_grouped_results(self, grouped_results):\n",
    "        \"\"\"Format grouped search results with hierarchical numbering\"\"\"\n",
    "        formatted = []\n",
    "        for group_idx, (query, results) in enumerate(grouped_results, 1):\n",
    "            if not results.empty:\n",
    "                formatted.append(f\"\\nğŸ” Results for: '{query}'\")\n",
    "                for item_idx, (_, row) in enumerate(results.iterrows(), 1):\n",
    "                    formatted.append(f\"{group_idx}.{item_idx} {row['title']} - {row['artist_name']}\")\n",
    "            else:\n",
    "                formatted.append(f\"\\nâš ï¸ No results found for: '{query}'\")\n",
    "        return \"\\n\".join(formatted)\n",
    "\n",
    "    def _parse_group_selection(self, selection, grouped_results):\n",
    "        \"\"\"Parse hierarchical selection like '1.1 2.3'\"\"\"\n",
    "        selected_ids = []\n",
    "        valid_groups = [g for g in grouped_results if not g[1].empty]\n",
    "        \n",
    "        for part in selection.split():\n",
    "            try:\n",
    "                group_num, item_num = map(int, part.split('.'))\n",
    "                # Adjust for valid groups only\n",
    "                if 1 <= group_num <= len(valid_groups):\n",
    "                    group_query, group_df = valid_groups[group_num-1]\n",
    "                    if 1 <= item_num <= len(group_df):\n",
    "                        selected_ids.append(group_df.iloc[item_num-1]['song_id'])\n",
    "            except:\n",
    "                continue\n",
    "        return selected_ids\n",
    "    \n",
    "    def generate_recommendations(self, input_titles, top_n=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Core recommendation generation function (Fixed Version)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Process each query separately\n",
    "            grouped_results = []\n",
    "            valid_queries = 0\n",
    "\n",
    "            for query in input_titles:\n",
    "                query = query.strip()\n",
    "                if not query:\n",
    "                    continue\n",
    "\n",
    "                results = self.search_songs(query)\n",
    "                grouped_results.append((query, results))\n",
    "                if not results.empty:\n",
    "                    valid_queries += 1\n",
    "\n",
    "            # Step 2: Display grouped results\n",
    "            if verbose:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(self._format_grouped_results(grouped_results))\n",
    "                print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # Step 3: Interactive selection\n",
    "            selected_ids = []\n",
    "            while True:\n",
    "                try:\n",
    "                    selection = input(\n",
    "                        \"Enter selections (e.g. '1.1 2.3'), '0' to re-search, or enter to confirm: \"\n",
    "                    ).strip()\n",
    "                    \n",
    "                    if selection == '0':\n",
    "                        return None\n",
    "                    if not selection:\n",
    "                        break\n",
    "                        \n",
    "                    selected_ids = self._parse_group_selection(selection, grouped_results)\n",
    "                    if not selected_ids:\n",
    "                        print(\"âš ï¸ No valid selections, try again\")\n",
    "                        continue\n",
    "                    break\n",
    "                        \n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"\\nâ¹ Selection canceled\")\n",
    "                    if input(\"Continue? (y/n): \").lower() == 'n':\n",
    "                        return pd.DataFrame()\n",
    "\n",
    "            # Step 4: Create virtual user\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(\"\\nâ­ Analyzing song features...\")\n",
    "                \n",
    "                virtual_user = self.create_virtual_user(selected_ids)\n",
    "            except ValueError as e:\n",
    "                if verbose:\n",
    "                    print(f\"âŒ Feature analysis failed: {str(e)}\")\n",
    "                return pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"âŒ Unexpected error: {str(e)}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Step 5: Calculate similarity (Fixed)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(\"ğŸ”¢ Calculating similarities...\")\n",
    "                \n",
    "                scores = np.dot(self.song_embeddings, virtual_user)\n",
    "                \n",
    "                # ä½¿ç”¨å½“å‰é€‰æ‹©çš„IDæ¥æ’é™¤å·²é€‰æ­Œæ›²\n",
    "                input_indices = [\n",
    "                    self.song_id_to_idx[sid] \n",
    "                    for sid in selected_ids  # ä½¿ç”¨å®é™…é€‰æ‹©çš„IDè€Œä¸æ˜¯all_matches\n",
    "                    if sid in self.song_id_to_idx\n",
    "                ]\n",
    "                scores[input_indices] = -np.inf  # æ’é™¤å·²é€‰æ­Œæ›²\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"âŒ Similarity calculation failed: {str(e)}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Step 6: Generate recommendations\n",
    "            try:\n",
    "                top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "                top_scores = scores[top_indices]\n",
    "                top_song_ids = self.song_encoder.inverse_transform(top_indices)\n",
    "\n",
    "                recommendations = self.metadata[\n",
    "                    self.metadata['song_id'].isin(top_song_ids)\n",
    "                ].copy()\n",
    "                \n",
    "                try:\n",
    "                    recommendations['predicted_plays'] = np.clip(\n",
    "                        top_scores * self.max_play, \n",
    "                        0,\n",
    "                        None\n",
    "                    )\n",
    "                except:\n",
    "                    recommendations['predicted_plays'] = 0\n",
    "\n",
    "                return recommendations.sort_values('predicted_plays', ascending=False)\n",
    "\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"âŒ Recommendation generation failed: {str(e)}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nâ¹ Recommendation process interrupted\")\n",
    "            return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"â— Unhandled exception: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# ----------------------\n",
    "# Interactive Recommendation Flow\n",
    "# ----------------------\n",
    "def interactive_recommendation():\n",
    "    recommender = AdvancedMusicRecommender()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nğŸµ Please enter your favorite songs/artists (enter 'exit' to quit):\")\n",
    "        user_input = input(\"> \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        queries = [q.strip() for q in user_input.split(',')]\n",
    "        \n",
    "        while True:\n",
    "            result = recommender.generate_recommendations(queries)\n",
    "            if result is None:\n",
    "                break\n",
    "            elif not result.empty:\n",
    "                print(\"\\nğŸ§ Recommended songs for you:\")\n",
    "                print(result[['title', 'artist_name', 'predicted_plays']]\n",
    "                    .head(10).to_string(index=False))\n",
    "                break\n",
    "            else:\n",
    "                print(\"Unable to generate recommendations, please try different input\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_recommendation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
