{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b7c096",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172f9594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    user_id             song_id  plays\n",
      "0  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1\n",
      "1  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1\n",
      "2  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2\n",
      "3  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1\n",
      "4  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载播放记录（用户-歌曲-播放次数）\n",
    "triplets = pd.read_csv('train_triplets.txt', sep='\\t', header=None, names=['user_id', 'song_id', 'plays'])\n",
    "print(triplets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0f2b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标题缺失比例: 0.0\n",
      "示例数据:\n",
      "                                    user_id             song_id  plays  \\\n",
      "0  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1   \n",
      "1  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1   \n",
      "2  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2   \n",
      "3  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1   \n",
      "4  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1   \n",
      "\n",
      "                             title  \n",
      "0                         The Cove  \n",
      "1             Nothing from Nothing  \n",
      "2                  Entre Dos Aguas  \n",
      "3            Under Cold Blue Stars  \n",
      "4  Riot Radio (Soundtrack Version)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def load_metadata(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        songs_dataset = f['metadata']['songs']\n",
    "        \n",
    "        # 提取原始字节数据\n",
    "        song_ids_bytes = songs_dataset['song_id'][()]  # 字节数组\n",
    "        titles_bytes = songs_dataset['title'][()]      # 字节数组\n",
    "        \n",
    "        # 安全解码为 UTF-8（处理非法字符）\n",
    "        song_ids = [s.decode('utf-8', errors='ignore').strip() for s in song_ids_bytes]\n",
    "        titles = [t.decode('utf-8', errors='ignore').strip() for t in titles_bytes]\n",
    "        \n",
    "        # 构建 DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'song_id': song_ids,\n",
    "            'title': titles\n",
    "        })\n",
    "        \n",
    "        # 移除空 song_id\n",
    "        df = df[df['song_id'].str.len() > 0]\n",
    "    return df\n",
    "\n",
    "metadata = load_metadata('msd_summary_file.h5')\n",
    "\n",
    "# 合并数据\n",
    "merged_data = pd.merge(\n",
    "    triplets,\n",
    "    metadata,\n",
    "    on='song_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 验证结果\n",
    "print(\"标题缺失比例:\", merged_data['title'].isnull().mean())\n",
    "print(\"示例数据:\")\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094746",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3feedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🆕 创建新的MLP模型\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embed (Embedding)          (None, 1, 64)        65236352    user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "item_embed (Embedding)          (None, 1, 64)        24610944    item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 64)           0           user_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 64)           0           item_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          33024       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 64)           8256        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            65          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 89,921,537\n",
      "Trainable params: 89,921,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5256\\1599165615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;31m# 训练模型（epochs可根据需要调整）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m history = model.fit(\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# 设置较大值，依靠早停机制\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Multiply, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Flatten, \n",
    "    Concatenate, Dropout, Dense  \n",
    ")\n",
    "# ----------------------\n",
    "# 数据预处理\n",
    "# ----------------------\n",
    "\n",
    "# 1. 用户和歌曲ID编码（转换为连续整数）\n",
    "user_encoder = LabelEncoder()\n",
    "song_encoder = LabelEncoder()\n",
    "\n",
    "# 对用户ID和歌曲ID进行编码\n",
    "merged_data['user_id_encoded'] = user_encoder.fit_transform(merged_data['user_id'])\n",
    "merged_data['song_id_encoded'] = song_encoder.fit_transform(merged_data['song_id'])\n",
    "\n",
    "# 2. 归一化播放次数到 [0,1]\n",
    "max_play = merged_data['plays'].max()\n",
    "merged_data['plays_normalized'] = merged_data['plays'] / max_play\n",
    "\n",
    "# 3. 提取训练数据\n",
    "user_ids = merged_data['user_id_encoded'].values\n",
    "item_ids = merged_data['song_id_encoded'].values\n",
    "labels = merged_data['plays_normalized'].values  # 归一化后的播放次数\n",
    "\n",
    "# 4. 划分训练集和测试集\n",
    "train_user, test_user, train_item, test_item, train_label, test_label = train_test_split(\n",
    "    user_ids, item_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 模型构建\n",
    "# ----------------------\n",
    "\n",
    "# 定义用户和物品数量\n",
    "num_users = len(user_encoder.classes_)  # 105,283\n",
    "num_items = len(song_encoder.classes_)  # 384,546\n",
    "embedding_size = 32  # 嵌入维度\n",
    "\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "def build_mlp_model():\n",
    "    pretrained_path = \"best_mlp_model.keras\"\n",
    "    \n",
    "    if Path(pretrained_path).exists():\n",
    "        print(\"⏳ 检测到预训练MLP模型，加载中...\")\n",
    "        model = tf.keras.models.load_model(pretrained_path)\n",
    "        print(\"✅ 成功加载预训练模型\")\n",
    "        \n",
    "        # 调整学习率\n",
    "        new_learning_rate = 0.0001\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=new_learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    else:\n",
    "        print(\"🆕 创建新的MLP模型\")\n",
    "        # 输入层\n",
    "        user_input = Input(shape=(1,), dtype=tf.int32, name='user_input')\n",
    "        item_input = Input(shape=(1,), dtype=tf.int32, name='item_input')\n",
    "        \n",
    "        # 嵌入层（适当增加维度）\n",
    "        user_embed = Embedding(num_users, 64, name='user_embed')(user_input)\n",
    "        item_embed = Embedding(num_items, 64, name='item_embed')(item_input)\n",
    "        \n",
    "        # 展平嵌入向量\n",
    "        user_flatten = Flatten()(user_embed)\n",
    "        item_flatten = Flatten()(item_embed)\n",
    "        \n",
    "        # 拼接特征\n",
    "        concat = Concatenate()([user_flatten, item_flatten])\n",
    "        \n",
    "        # 深度全连接层\n",
    "        dense = Dense(256, activation='relu')(concat)\n",
    "        dense = Dropout(0.2)(dense)  # 添加Dropout防止过拟合\n",
    "        dense = Dense(128, activation='relu')(dense)\n",
    "        dense = Dense(64, activation='relu')(dense)\n",
    "        \n",
    "        # 输出层（回归任务使用线性激活）\n",
    "        output = Dense(1, activation='linear')(dense)\n",
    "        \n",
    "        model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "# ----------------------\n",
    "# 修改训练流程（替换模型构建部分）\n",
    "# ----------------------\n",
    "model = build_mlp_model()\n",
    "model.summary()\n",
    "\n",
    "# 添加模型保存回调（自动保存最佳模型）\n",
    "class CustomCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # 添加优化器状态保存路径\n",
    "        self.optimizer_path = \"optimizer_state_mlp.pkl\"\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        # 保存优化器权重\n",
    "        joblib.dump(self.model.optimizer.get_weights(), self.optimizer_path)\n",
    "        print(f\"💾 已保存优化器状态至 {self.optimizer_path}\")\n",
    "\n",
    "# 更新检查点路径\n",
    "checkpoint = CustomCheckpoint(\n",
    "    \"best_mlp_model.keras\",  # 修改保存文件名\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 如果检测到优化器状态则加载\n",
    "if Path(\"optimizer_state_mlp.pkl\").exists():\n",
    "    print(\"⏳ 加载优化器状态...\")\n",
    "    optimizer_weights = joblib.load(\"optimizer_state_mlp.pkl\")\n",
    "    model.optimizer.set_weights(optimizer_weights)\n",
    "    print(\"✅ 优化器状态已恢复\")\n",
    "\n",
    "# 训练模型（epochs可根据需要调整）\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=10,  # 设置较大值，依靠早停机制\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 模型评估与预测\n",
    "# ----------------------\n",
    "\n",
    "# 评估测试集\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f\"testsets MSE: {test_loss:.4f}, MAE: {test_mae:.4f}\")\n",
    "\n",
    "# 保存编码器（训练后立即执行）\n",
    "joblib.dump(max_play, 'max_play_mlp.pkl')\n",
    "joblib.dump(user_encoder, 'user_encoder_mlp.pkl')\n",
    "joblib.dump(song_encoder, 'song_encoder_mlp.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import h5py\n",
    "# ----------------------\n",
    "# Load metadata \n",
    "# ----------------------\n",
    "def load_metadata(filename):\n",
    "    \"\"\"Metadata loading function identical to the training code\"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        songs_dataset = f['metadata/songs']  # Note the hierarchy uses '/' separators\n",
    "        \n",
    "        # Extract all required fields\n",
    "        song_ids_bytes = songs_dataset['song_id'][()]\n",
    "        titles_bytes = songs_dataset['title'][()]\n",
    "        artists_bytes = songs_dataset['artist_name'][()]  # Added artist field\n",
    "        \n",
    "        # Unified decoding process\n",
    "        decode_func = lambda x: x.decode('utf-8', errors='ignore').strip()\n",
    "        song_ids = list(map(decode_func, song_ids_bytes))\n",
    "        titles = list(map(decode_func, titles_bytes))\n",
    "        artists = list(map(decode_func, artists_bytes))\n",
    "        \n",
    "        # Build DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'song_id': song_ids,\n",
    "            'title': titles,\n",
    "            'artist_name': artists  # Add other required fields\n",
    "        })\n",
    "        \n",
    "        # Filter invalid data\n",
    "        return df[df['song_id'].str.len() > 0]\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "\n",
    "# Enhanced Recommendation System Class\n",
    "# ----------------------\n",
    "class AdvancedMusicRecommender:\n",
    "    def __init__(self):\n",
    "        # Load metadata\n",
    "        self.metadata = load_metadata('msd_summary_file.h5')\n",
    "        \n",
    "        # Load model and encoders\n",
    "        self.model = tf.keras.models.load_model('best_mlp_model.keras')\n",
    "        self.song_encoder = joblib.load('song_encoder_mlp.pkl')\n",
    "        \n",
    "        # Handle missing max_play\n",
    "        try:\n",
    "            self.max_play = joblib.load('max_play_mlp.pkl')\n",
    "        except FileNotFoundError:\n",
    "            print(\"Warning: Estimating max_play using metadata\")\n",
    "            self.max_play = self.metadata['plays'].max() if 'plays' in self.metadata else 1\n",
    "        \n",
    "        # Create song ID to index mapping\n",
    "        self.song_id_to_idx = {\n",
    "            song_id: idx \n",
    "            for idx, song_id in enumerate(self.song_encoder.classes_)\n",
    "        }\n",
    "        \n",
    "        # Get song embeddings\n",
    "        self.song_embeddings = self.model.get_layer('item_embed').get_weights()[0]\n",
    "\n",
    "    def search_songs(self, query, top_k=5):\n",
    "        \"\"\"Modified search function with consistent fields\"\"\"\n",
    "        mask = (\n",
    "            self.metadata['title'].str.contains(query, case=False) |\n",
    "            self.metadata['artist_name'].str.contains(query, case=False)\n",
    "        )\n",
    "        return self.metadata[mask].head(top_k)[['song_id', 'title', 'artist_name']]\n",
    "    \n",
    "    def create_virtual_user(self, song_ids):\n",
    "        \"\"\"Create virtual user features from song IDs\"\"\"\n",
    "        valid_ids = [song_id for song_id in song_ids if song_id in self.song_id_to_idx]\n",
    "        \n",
    "        if not valid_ids:\n",
    "            raise ValueError(\"No valid song IDs found\")\n",
    "            \n",
    "        indices = [self.song_id_to_idx[song_id] for song_id in valid_ids]\n",
    "        avg_embedding = np.mean(self.song_embeddings[indices], axis=0)\n",
    "        return avg_embedding\n",
    "\n",
    "    def _select_songs_interactively(self, matched_songs):\n",
    "        \"\"\"Interactive song selection with re-search option\"\"\"\n",
    "        print(\"\\n🔍 Found matching songs:\")\n",
    "        print(\"0. Search again (unsatisfied with results)\")\n",
    "        for idx, (_, row) in enumerate(matched_songs.iterrows(), 1):\n",
    "            print(f\"{idx}. {row['title']} - {row['artist_name']}\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                selected = input(\"Enter song numbers (space-separated, 0 to re-search, enter for all): \").strip()\n",
    "                if not selected:\n",
    "                    return matched_songs['song_id'].tolist()\n",
    "                \n",
    "                if '0' in selected.split():\n",
    "                    return None\n",
    "                \n",
    "                indices = list(map(int, selected.split()))\n",
    "                valid_indices = [i for i in indices if 1 <= i <= len(matched_songs)]\n",
    "                \n",
    "                if not valid_indices:\n",
    "                    print(\"⚠️ Invalid input, please try again\")\n",
    "                    continue\n",
    "                \n",
    "                return matched_songs.iloc[[i-1 for i in valid_indices]]['song_id'].tolist()\n",
    "            \n",
    "            except ValueError:\n",
    "                print(\"⚠️ Please enter valid numbers\")\n",
    "\n",
    "    def _format_grouped_results(self, grouped_results):\n",
    "        \"\"\"Format grouped search results with hierarchical numbering\"\"\"\n",
    "        formatted = []\n",
    "        for group_idx, (query, results) in enumerate(grouped_results, 1):\n",
    "            if not results.empty:\n",
    "                formatted.append(f\"\\n🔍 Results for: '{query}'\")\n",
    "                for item_idx, (_, row) in enumerate(results.iterrows(), 1):\n",
    "                    formatted.append(f\"{group_idx}.{item_idx} {row['title']} - {row['artist_name']}\")\n",
    "            else:\n",
    "                formatted.append(f\"\\n⚠️ No results found for: '{query}'\")\n",
    "        return \"\\n\".join(formatted)\n",
    "\n",
    "    def _parse_group_selection(self, selection, grouped_results):\n",
    "        \"\"\"Parse hierarchical selection like '1.1 2.3'\"\"\"\n",
    "        selected_ids = []\n",
    "        valid_groups = [g for g in grouped_results if not g[1].empty]\n",
    "        \n",
    "        for part in selection.split():\n",
    "            try:\n",
    "                group_num, item_num = map(int, part.split('.'))\n",
    "                # Adjust for valid groups only\n",
    "                if 1 <= group_num <= len(valid_groups):\n",
    "                    group_query, group_df = valid_groups[group_num-1]\n",
    "                    if 1 <= item_num <= len(group_df):\n",
    "                        selected_ids.append(group_df.iloc[item_num-1]['song_id'])\n",
    "            except:\n",
    "                continue\n",
    "        return selected_ids\n",
    "    \n",
    "    def generate_recommendations(self, input_titles, top_n=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Core recommendation generation function (Fixed Version)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Process each query separately\n",
    "            grouped_results = []\n",
    "            valid_queries = 0\n",
    "\n",
    "            for query in input_titles:\n",
    "                query = query.strip()\n",
    "                if not query:\n",
    "                    continue\n",
    "\n",
    "                results = self.search_songs(query)\n",
    "                grouped_results.append((query, results))\n",
    "                if not results.empty:\n",
    "                    valid_queries += 1\n",
    "\n",
    "            # Step 2: Display grouped results\n",
    "            if verbose:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(self._format_grouped_results(grouped_results))\n",
    "                print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            # Step 3: Interactive selection\n",
    "            selected_ids = []\n",
    "            while True:\n",
    "                try:\n",
    "                    selection = input(\n",
    "                        \"Enter selections (e.g. '1.1 2.3'), '0' to re-search, or enter to confirm: \"\n",
    "                    ).strip()\n",
    "                    \n",
    "                    if selection == '0':\n",
    "                        return None\n",
    "                    if not selection:\n",
    "                        break\n",
    "                        \n",
    "                    selected_ids = self._parse_group_selection(selection, grouped_results)\n",
    "                    if not selected_ids:\n",
    "                        print(\"⚠️ No valid selections, try again\")\n",
    "                        continue\n",
    "                    break\n",
    "                        \n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"\\n⏹ Selection canceled\")\n",
    "                    if input(\"Continue? (y/n): \").lower() == 'n':\n",
    "                        return pd.DataFrame()\n",
    "\n",
    "            # Step 4: Create virtual user\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(\"\\n⭐ Analyzing song features...\")\n",
    "                \n",
    "                virtual_user = self.create_virtual_user(selected_ids)\n",
    "            except ValueError as e:\n",
    "                if verbose:\n",
    "                    print(f\"❌ Feature analysis failed: {str(e)}\")\n",
    "                return pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"❌ Unexpected error: {str(e)}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Step 5: Calculate similarity (Fixed)\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(\"🔢 Calculating similarities...\")\n",
    "                \n",
    "                scores = np.dot(self.song_embeddings, virtual_user)\n",
    "                \n",
    "                # 使用当前选择的ID来排除已选歌曲\n",
    "                input_indices = [\n",
    "                    self.song_id_to_idx[sid] \n",
    "                    for sid in selected_ids  # 使用实际选择的ID而不是all_matches\n",
    "                    if sid in self.song_id_to_idx\n",
    "                ]\n",
    "                scores[input_indices] = -np.inf  # 排除已选歌曲\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"❌ Similarity calculation failed: {str(e)}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Step 6: Generate recommendations\n",
    "            try:\n",
    "                top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "                top_scores = scores[top_indices]\n",
    "                top_song_ids = self.song_encoder.inverse_transform(top_indices)\n",
    "\n",
    "                recommendations = self.metadata[\n",
    "                    self.metadata['song_id'].isin(top_song_ids)\n",
    "                ].copy()\n",
    "                \n",
    "                try:\n",
    "                    recommendations['predicted_plays'] = np.clip(\n",
    "                        top_scores * self.max_play, \n",
    "                        0,\n",
    "                        None\n",
    "                    )\n",
    "                except:\n",
    "                    recommendations['predicted_plays'] = 0\n",
    "\n",
    "                return recommendations.sort_values('predicted_plays', ascending=False)\n",
    "\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"❌ Recommendation generation failed: {str(e)}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n⏹ Recommendation process interrupted\")\n",
    "            return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"❗ Unhandled exception: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# ----------------------\n",
    "# Interactive Recommendation Flow\n",
    "# ----------------------\n",
    "def interactive_recommendation():\n",
    "    recommender = AdvancedMusicRecommender()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n🎵 Please enter your favorite songs/artists (enter 'exit' to quit):\")\n",
    "        user_input = input(\"> \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        queries = [q.strip() for q in user_input.split(',')]\n",
    "        \n",
    "        while True:\n",
    "            result = recommender.generate_recommendations(queries)\n",
    "            if result is None:\n",
    "                break\n",
    "            elif not result.empty:\n",
    "                print(\"\\n🎧 Recommended songs for you:\")\n",
    "                print(result[['title', 'artist_name', 'predicted_plays']]\n",
    "                    .head(10).to_string(index=False))\n",
    "                break\n",
    "            else:\n",
    "                print(\"Unable to generate recommendations, please try different input\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_recommendation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
