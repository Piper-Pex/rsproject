{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b7c096",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172f9594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    user_id             song_id  plays\n",
      "0  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1\n",
      "1  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1\n",
      "2  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2\n",
      "3  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1\n",
      "4  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åŠ è½½æ’­æ”¾è®°å½•ï¼ˆç”¨æˆ·-æ­Œæ›²-æ’­æ”¾æ¬¡æ•°ï¼‰\n",
    "triplets = pd.read_csv('train_triplets.txt', sep='\\t', header=None, names=['user_id', 'song_id', 'plays'])\n",
    "print(triplets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0f2b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ‡é¢˜ç¼ºå¤±æ¯”ä¾‹: 0.0\n",
      "ç¤ºä¾‹æ•°æ®:\n",
      "                                    user_id             song_id  plays  \\\n",
      "0  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1   \n",
      "1  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1   \n",
      "2  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2   \n",
      "3  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1   \n",
      "4  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1   \n",
      "\n",
      "                             title  \n",
      "0                         The Cove  \n",
      "1             Nothing from Nothing  \n",
      "2                  Entre Dos Aguas  \n",
      "3            Under Cold Blue Stars  \n",
      "4  Riot Radio (Soundtrack Version)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def load_metadata(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        songs_dataset = f['metadata']['songs']\n",
    "        \n",
    "        # æå–åŸå§‹å­—èŠ‚æ•°æ®\n",
    "        song_ids_bytes = songs_dataset['song_id'][()]  # å­—èŠ‚æ•°ç»„\n",
    "        titles_bytes = songs_dataset['title'][()]      # å­—èŠ‚æ•°ç»„\n",
    "        \n",
    "        # å®‰å…¨è§£ç ä¸º UTF-8ï¼ˆå¤„ç†éæ³•å­—ç¬¦ï¼‰\n",
    "        song_ids = [s.decode('utf-8', errors='ignore').strip() for s in song_ids_bytes]\n",
    "        titles = [t.decode('utf-8', errors='ignore').strip() for t in titles_bytes]\n",
    "        \n",
    "        # æ„å»º DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'song_id': song_ids,\n",
    "            'title': titles\n",
    "        })\n",
    "        \n",
    "        # ç§»é™¤ç©º song_id\n",
    "        df = df[df['song_id'].str.len() > 0]\n",
    "    return df\n",
    "\n",
    "metadata = load_metadata('msd_summary_file.h5')\n",
    "\n",
    "# åˆå¹¶æ•°æ®\n",
    "merged_data = pd.merge(\n",
    "    triplets,\n",
    "    metadata,\n",
    "    on='song_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# éªŒè¯ç»“æœ\n",
    "print(\"æ ‡é¢˜ç¼ºå¤±æ¯”ä¾‹:\", merged_data['title'].isnull().mean())\n",
    "print(\"ç¤ºä¾‹æ•°æ®:\")\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094746",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3feedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ†• åˆ›å»ºFusionèåˆæ¨¡å‹\n",
      "ğŸ’¡ åŠ è½½é¢„è®­ç»ƒGMFç»„ä»¶\n",
      "ğŸ’¡ åŠ è½½é¢„è®­ç»ƒMLPç»„ä»¶\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_user_embed (Embeddin (None, 1, 64)        65236352    user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_item_embed (Embeddin (None, 1, 64)        24610944    item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_user_flatten (Flatte (None, 64)           0           fusion_mlp_user_embed[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_item_flatten (Flatte (None, 64)           0           fusion_mlp_item_embed[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_concat (Concatenate) (None, 128)          0           fusion_mlp_user_flatten[0][0]    \n",
      "                                                                 fusion_mlp_item_flatten[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_dense1 (Dense)       (None, 256)          33024       fusion_mlp_concat[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "fusion_gmf_user_embed (Embeddin (None, 1, 16)        16309088    user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fusion_gmf_item_embed (Embeddin (None, 1, 16)        6152736     item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_dropout (Dropout)    (None, 256)          0           fusion_mlp_dense1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "fusion_gmf_mul (Multiply)       (None, 1, 16)        0           fusion_gmf_user_embed[0][0]      \n",
      "                                                                 fusion_gmf_item_embed[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_dense2 (Dense)       (None, 128)          32896       fusion_mlp_dropout[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "fusion_gmf_flatten (Flatten)    (None, 16)           0           fusion_gmf_mul[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_output (Dense)       (None, 64)           8256        fusion_mlp_dense2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "fusion_concat (Concatenate)     (None, 80)           0           fusion_gmf_flatten[0][0]         \n",
      "                                                                 fusion_mlp_output[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "fusion_dense (Dense)            (None, 32)           2592        fusion_concat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fusion_output (Dense)           (None, 1)            33          fusion_dense[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 112,385,921\n",
      "Trainable params: 112,385,921\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "2426/2426 [==============================] - 83s 34ms/step - loss: 5.7579e-07 - mae: 2.9398e-04 - val_loss: 5.3541e-07 - val_mae: 2.6501e-04\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00000, saving model to best_fusion_model.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Piper\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "2426/2426 [==============================] - 74s 31ms/step - loss: 4.2696e-07 - mae: 2.5634e-04 - val_loss: 5.0945e-07 - val_mae: 2.4500e-04\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to best_fusion_model.keras\n",
      "Epoch 3/10\n",
      "2426/2426 [==============================] - 75s 31ms/step - loss: 4.2092e-07 - mae: 2.5702e-04 - val_loss: 5.4190e-07 - val_mae: 2.5097e-04\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "Epoch 4/10\n",
      "2426/2426 [==============================] - 75s 31ms/step - loss: 4.2384e-07 - mae: 2.5995e-04 - val_loss: 5.3875e-07 - val_mae: 2.6131e-04\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "Epoch 5/10\n",
      "2426/2426 [==============================] - 95s 39ms/step - loss: 4.2399e-07 - mae: 2.5658e-04 - val_loss: 5.3580e-07 - val_mae: 2.6433e-04\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "Epoch 00005: early stopping\n",
      "ğŸ’¾ å·²ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€è‡³ optimizer_state_fusion.pkl\n",
      "500/607 [=======================>......] - ETA: 2s - loss: 5.2583e-07 - mae: 2.4496e-04"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Multiply, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Flatten, \n",
    "    Concatenate, Dropout, Dense  \n",
    ")\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "# ----------------------\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "# ----------------------\n",
    "\n",
    "# 1. ç”¨æˆ·å’Œæ­Œæ›²IDç¼–ç ï¼ˆè½¬æ¢ä¸ºè¿ç»­æ•´æ•°ï¼‰\n",
    "user_encoder = LabelEncoder()\n",
    "song_encoder = LabelEncoder()\n",
    "\n",
    "# å¯¹ç”¨æˆ·IDå’Œæ­Œæ›²IDè¿›è¡Œç¼–ç \n",
    "merged_data['user_id_encoded'] = user_encoder.fit_transform(merged_data['user_id'])\n",
    "merged_data['song_id_encoded'] = song_encoder.fit_transform(merged_data['song_id'])\n",
    "\n",
    "# 2. å½’ä¸€åŒ–æ’­æ”¾æ¬¡æ•°åˆ° [0,1]\n",
    "max_play = merged_data['plays'].max()\n",
    "merged_data['plays_normalized'] = merged_data['plays'] / max_play\n",
    "\n",
    "# 3. æå–è®­ç»ƒæ•°æ®\n",
    "user_ids = merged_data['user_id_encoded'].values\n",
    "item_ids = merged_data['song_id_encoded'].values\n",
    "labels = merged_data['plays_normalized'].values  # å½’ä¸€åŒ–åçš„æ’­æ”¾æ¬¡æ•°\n",
    "\n",
    "# 4. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_user, test_user, train_item, test_item, train_label, test_label = train_test_split(\n",
    "    user_ids, item_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# æ¨¡å‹æ„å»º\n",
    "# ----------------------\n",
    "\n",
    "# å®šä¹‰ç”¨æˆ·å’Œç‰©å“æ•°é‡\n",
    "num_users = len(user_encoder.classes_)  # 105,283\n",
    "num_items = len(song_encoder.classes_)  # 384,546\n",
    "embedding_size = 32  # åµŒå…¥ç»´åº¦\n",
    "\n",
    "\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "def build_fusion_model(gmf_model=None, mlp_model=None):\n",
    "    pretrained_path = \"best_fusion_model.keras\"\n",
    "    \n",
    "    if Path(pretrained_path).exists():\n",
    "        print(\"â³ æ£€æµ‹åˆ°é¢„è®­ç»ƒFusionæ¨¡å‹ï¼ŒåŠ è½½ä¸­...\")\n",
    "        model = tf.keras.models.load_model(pretrained_path)\n",
    "        print(\"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹\")\n",
    "        return model\n",
    "    \n",
    "    print(\"ğŸ†• åˆ›å»ºFusionèåˆæ¨¡å‹\")\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "    def safe_load_submodel(path, prefix):\n",
    "        model = tf.keras.models.load_model(path)\n",
    "        for layer in model.layers:\n",
    "            layer._name = f\"{prefix}_{layer.name}\"\n",
    "            if isinstance(layer, tf.keras.Model):\n",
    "                for sublayer in layer.layers:\n",
    "                    sublayer._name = f\"{prefix}_{sublayer.name}\"\n",
    "        return model\n",
    "\n",
    "    # GMFåˆ†æ”¯\n",
    "    if gmf_model and Path(gmf_model).exists():\n",
    "        print(\"ğŸ’¡ åŠ è½½é¢„è®­ç»ƒGMFç»„ä»¶\")\n",
    "        # 1. å®‰å…¨åŠ è½½å­æ¨¡å‹å¹¶ç»Ÿä¸€åŠ å‰ç¼€\n",
    "        gmf_submodel = safe_load_submodel(gmf_model, \"gmf\")\n",
    "        # 2. æå–é¢„è®­ç»ƒçš„ Embedding æƒé‡\n",
    "        gmf_user_weights = gmf_submodel.get_layer(\"gmf_user_embed\").get_weights()\n",
    "        gmf_item_weights = gmf_submodel.get_layer(\"gmf_item_embed\").get_weights()\n",
    "        # 3. ç”¨æå–åˆ°çš„æƒé‡æ„å»ºèåˆç”¨ Embeddingï¼Œå¹¶åŠ è½½æƒé‡\n",
    "        fusion_gmf_user_embed = Embedding(\n",
    "            num_users, 16,\n",
    "            name=\"fusion_gmf_user_embed\",\n",
    "            \n",
    "            weights=gmf_user_weights,\n",
    "            trainable=True\n",
    "        )(user_input)\n",
    "        fusion_gmf_item_embed = Embedding(\n",
    "            num_items, 16,\n",
    "            name=\"fusion_gmf_item_embed\",\n",
    "            weights=gmf_item_weights,\n",
    "            trainable=True\n",
    "        )(item_input)\n",
    "        # 4. é€å…ƒç´ ç›¸ä¹˜ + æ‹‰å¹³\n",
    "        fusion_gmf_mul     = Multiply(name=\"fusion_gmf_mul\")([fusion_gmf_user_embed, fusion_gmf_item_embed])\n",
    "        gmf_flatten        = Flatten(name=\"fusion_gmf_flatten\")(fusion_gmf_mul)\n",
    "    else:\n",
    "        print(\"ğŸ’¡ åˆå§‹åŒ–æ–°GMFåˆ†æ”¯\")\n",
    "        gmf_user_embed = Embedding(num_users, 16, name='gmf_user_embed')(user_input)\n",
    "        gmf_item_embed = Embedding(num_items, 16, name='gmf_item_embed')(item_input)\n",
    "        gmf_output = Multiply()([Flatten()(gmf_user_embed), Flatten()(gmf_item_embed)])\n",
    "        gmf_flatten = Flatten(name='gmf_flatten')(gmf_output)\n",
    "    # MLPåˆ†æ”¯\n",
    "    if mlp_model and Path(mlp_model).exists():\n",
    "        print(\"ğŸ’¡ åŠ è½½é¢„è®­ç»ƒMLPç»„ä»¶\")\n",
    "        # 1. å®‰å…¨åŠ è½½å­æ¨¡å‹å¹¶ç»Ÿä¸€åŠ å‰ç¼€\n",
    "        mlp_submodel = safe_load_submodel(mlp_model, \"mlp\")\n",
    "        # 2. æå–é¢„è®­ç»ƒçš„ Embedding æƒé‡\n",
    "        mlp_user_weights = mlp_submodel.get_layer(\"mlp_user_embed\").get_weights()\n",
    "        mlp_item_weights = mlp_submodel.get_layer(\"mlp_item_embed\").get_weights()\n",
    "        # 3. ç”¨æå–åˆ°çš„æƒé‡æ„å»ºèåˆç”¨ Embeddingï¼Œå¹¶åŠ è½½æƒé‡\n",
    "        fusion_mlp_user_embed      = Embedding(\n",
    "            num_users, 64,\n",
    "            name=\"fusion_mlp_user_embed\",\n",
    "            weights=mlp_user_weights,\n",
    "            trainable=True\n",
    "        )(user_input)\n",
    "        fusion_mlp_item_embed      = Embedding(\n",
    "            num_items, 64,\n",
    "            name=\"fusion_mlp_item_embed\",\n",
    "            weights=mlp_item_weights,\n",
    "            trainable=True\n",
    "        )(item_input)\n",
    "        # 4. å±•å¹³å†æ‹¼æ¥\n",
    "        fusion_mlp_user_flatten    = Flatten(name=\"fusion_mlp_user_flatten\")(fusion_mlp_user_embed)\n",
    "        fusion_mlp_item_flatten    = Flatten(name=\"fusion_mlp_item_flatten\")(fusion_mlp_item_embed)\n",
    "        mlp_concat                 = Concatenate(name=\"fusion_mlp_concat\")(\n",
    "            [fusion_mlp_user_flatten, fusion_mlp_item_flatten]\n",
    "        )\n",
    "        # 5. æ–°çš„ MLP éšè—å±‚\n",
    "        fusion_mlp_dense1          = Dense(\n",
    "            256, activation=\"relu\", name=\"fusion_mlp_dense1\"\n",
    "        )(mlp_concat)\n",
    "        fusion_mlp_dropout         = Dropout(0.2, name=\"fusion_mlp_dropout\")(fusion_mlp_dense1)\n",
    "        fusion_mlp_dense2          = Dense(\n",
    "            128, activation=\"relu\", name=\"fusion_mlp_dense2\"\n",
    "        )(fusion_mlp_dropout)\n",
    "        mlp_output                 = Dense(\n",
    "            64, activation=\"relu\", name=\"fusion_mlp_output\"\n",
    "        )(fusion_mlp_dense2)\n",
    "    else:\n",
    "        print(\"ğŸ’¡ åˆå§‹åŒ–æ–°MLPåˆ†æ”¯\")\n",
    "        mlp_user_embed = Embedding(num_users, 64, name='mlp_user_embed')(user_input)\n",
    "        mlp_item_embed = Embedding(num_items, 64, name='mlp_item_embed')(item_input)\n",
    "        mlp_concat = Concatenate()([Flatten()(mlp_user_embed), Flatten()(mlp_item_embed)])\n",
    "        mlp_dense = Dense(256, activation='relu')(mlp_concat)\n",
    "        mlp_dense = Dropout(0.2)(mlp_dense)\n",
    "        mlp_dense = Dense(128, activation='relu')(mlp_dense)\n",
    "        mlp_output = Dense(64, activation='relu')(mlp_dense)\n",
    "\n",
    "    # èåˆå±‚\n",
    "    merged = Concatenate(name='fusion_concat')([gmf_flatten, mlp_output])\n",
    "    final_dense = Dense(32, activation='relu', name='fusion_dense')(merged)\n",
    "    output = Dense(1, activation='linear', name='fusion_output')(final_dense)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae'],\n",
    "        steps_per_execution=50,  # æ¯æ¬¡ tf.function è°ƒç”¨è·‘ 50 ä¸ª batch\n",
    "    )\n",
    "    return model\n",
    "# ----------------------\n",
    "# åˆå§‹åŒ–å¹¶è®­ç»ƒèåˆæ¨¡å‹\n",
    "# ----------------------\n",
    "# åŠ è½½é¢„è®­ç»ƒç»„ä»¶ï¼ˆå¯é€‰ï¼‰\n",
    "model = build_fusion_model(\n",
    "    gmf_model=\"best_gmf_model.keras\",\n",
    "    mlp_model=\"best_mlp_model.keras\"\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "batch_size = 16384\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((train_user, train_item), train_label)\n",
    ").cache().shuffle(100000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((test_user, test_item), test_label)\n",
    ").cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "# æ·»åŠ æ¨¡å‹ä¿å­˜å›è°ƒï¼ˆè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰\n",
    "class CustomCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # æ·»åŠ ä¼˜åŒ–å™¨çŠ¶æ€ä¿å­˜è·¯å¾„\n",
    "        self.optimizer_path = \"optimizer_state_fusion.pkl\"\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        # ä¿å­˜ä¼˜åŒ–å™¨æƒé‡\n",
    "        joblib.dump(self.model.optimizer.get_weights(), self.optimizer_path)\n",
    "        print(f\"ğŸ’¾ å·²ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€è‡³ {self.optimizer_path}\")\n",
    "\n",
    "# æ›´æ–°æ£€æŸ¥ç‚¹è·¯å¾„\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True   # æ˜¯å¦æ¢å¤åˆ°æœ€ä½³æƒé‡\n",
    ")\n",
    "\n",
    "checkpoint = CustomCheckpoint(\n",
    "    \"best_fusion_model.keras\",  # ä¿®æ”¹ä¿å­˜æ–‡ä»¶å\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# å¦‚æœæ£€æµ‹åˆ°ä¼˜åŒ–å™¨çŠ¶æ€åˆ™åŠ è½½\n",
    "if Path(\"optimizer_state_fusion.pkl\").exists():\n",
    "    print(\"â³ åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€...\")\n",
    "    optimizer_weights = joblib.load(\"optimizer_state_fusion.pkl\")\n",
    "    model.optimizer.set_weights(optimizer_weights)\n",
    "    print(\"âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤\")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹ï¼ˆepochså¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰\n",
    "history = model.fit(\n",
    "    [train_user, train_item],             # ä¸¤ä¸ª numpy array\n",
    "    train_label,                          # æ ‡ç­¾\n",
    "    batch_size=16384,                     # æ ¹æ®æ˜¾å­˜å¯å¢å¤§/å‡å°\n",
    "    epochs=10,\n",
    "    validation_data=([test_user, test_item], test_label),\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1                             # batch çº§è¿›åº¦æ¡\n",
    ")\n",
    "# ----------------------\n",
    "# æ¨¡å‹è¯„ä¼°ä¸é¢„æµ‹\n",
    "# ----------------------\n",
    "\n",
    "# è¯„ä¼°æµ‹è¯•é›†\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f\"testsets MSE: {test_loss:.4f}, MAE: {test_mae:.4f}\")\n",
    "\n",
    "# ä¿å­˜ç¼–ç å™¨ï¼ˆè®­ç»ƒåç«‹å³æ‰§è¡Œï¼‰\n",
    "joblib.dump(max_play, 'max_play_fusion.pkl')\n",
    "joblib.dump(user_encoder, 'user_encoder_fusion.pkl')\n",
    "joblib.dump(song_encoder, 'song_encoder_fusion.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
