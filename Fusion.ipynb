{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b7c096",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172f9594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    user_id             song_id  plays\n",
      "0  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1\n",
      "1  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1\n",
      "2  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2\n",
      "3  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1\n",
      "4  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Âä†ËΩΩÊí≠ÊîæËÆ∞ÂΩïÔºàÁî®Êà∑-Ê≠åÊõ≤-Êí≠ÊîæÊ¨°Êï∞Ôºâ\n",
    "triplets = pd.read_csv('train_triplets.txt', sep='\\t', header=None, names=['user_id', 'song_id', 'plays'])\n",
    "print(triplets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0f2b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê†áÈ¢òÁº∫Â§±ÊØî‰æã: 0.0\n",
      "Á§∫‰æãÊï∞ÊçÆ:\n",
      "                                    user_id             song_id  plays  \\\n",
      "0  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1   \n",
      "1  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1   \n",
      "2  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2   \n",
      "3  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1   \n",
      "4  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1   \n",
      "\n",
      "                             title  \n",
      "0                         The Cove  \n",
      "1             Nothing from Nothing  \n",
      "2                  Entre Dos Aguas  \n",
      "3            Under Cold Blue Stars  \n",
      "4  Riot Radio (Soundtrack Version)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def load_metadata(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        songs_dataset = f['metadata']['songs']\n",
    "        \n",
    "        # ÊèêÂèñÂéüÂßãÂ≠óËäÇÊï∞ÊçÆ\n",
    "        song_ids_bytes = songs_dataset['song_id'][()]  # Â≠óËäÇÊï∞ÁªÑ\n",
    "        titles_bytes = songs_dataset['title'][()]      # Â≠óËäÇÊï∞ÁªÑ\n",
    "        \n",
    "        # ÂÆâÂÖ®Ëß£Á†Å‰∏∫ UTF-8ÔºàÂ§ÑÁêÜÈùûÊ≥ïÂ≠óÁ¨¶Ôºâ\n",
    "        song_ids = [s.decode('utf-8', errors='ignore').strip() for s in song_ids_bytes]\n",
    "        titles = [t.decode('utf-8', errors='ignore').strip() for t in titles_bytes]\n",
    "        \n",
    "        # ÊûÑÂª∫ DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'song_id': song_ids,\n",
    "            'title': titles\n",
    "        })\n",
    "        \n",
    "        # ÁßªÈô§Á©∫ song_id\n",
    "        df = df[df['song_id'].str.len() > 0]\n",
    "    return df\n",
    "\n",
    "metadata = load_metadata('msd_summary_file.h5')\n",
    "\n",
    "# ÂêàÂπ∂Êï∞ÊçÆ\n",
    "merged_data = pd.merge(\n",
    "    triplets,\n",
    "    metadata,\n",
    "    on='song_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# È™åËØÅÁªìÊûú\n",
    "print(\"Ê†áÈ¢òÁº∫Â§±ÊØî‰æã:\", merged_data['title'].isnull().mean())\n",
    "print(\"Á§∫‰æãÊï∞ÊçÆ:\")\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094746",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3feedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï ÂàõÂª∫FusionËûçÂêàÊ®°Âûã\n",
      "üí° Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉGMFÁªÑ‰ª∂\n",
      "üí° Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉMLPÁªÑ‰ª∂\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_user_embed (Embeddin (None, 1, 64)        65236352    user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_item_embed (Embeddin (None, 1, 64)        24610944    item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_user_flatten (Flatte (None, 64)           0           fusion_mlp_user_embed[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_item_flatten (Flatte (None, 64)           0           fusion_mlp_item_embed[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_concat (Concatenate) (None, 128)          0           fusion_mlp_user_flatten[0][0]    \n",
      "                                                                 fusion_mlp_item_flatten[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_dense1 (Dense)       (None, 256)          33024       fusion_mlp_concat[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "fusion_gmf_user_embed (Embeddin (None, 1, 16)        16309088    user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fusion_gmf_item_embed (Embeddin (None, 1, 16)        6152736     item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_dropout (Dropout)    (None, 256)          0           fusion_mlp_dense1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "fusion_gmf_mul (Multiply)       (None, 1, 16)        0           fusion_gmf_user_embed[0][0]      \n",
      "                                                                 fusion_gmf_item_embed[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_dense2 (Dense)       (None, 128)          32896       fusion_mlp_dropout[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "fusion_gmf_flatten (Flatten)    (None, 16)           0           fusion_gmf_mul[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fusion_mlp_output (Dense)       (None, 64)           8256        fusion_mlp_dense2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "fusion_concat (Concatenate)     (None, 80)           0           fusion_gmf_flatten[0][0]         \n",
      "                                                                 fusion_mlp_output[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "fusion_dense (Dense)            (None, 32)           2592        fusion_concat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fusion_output (Dense)           (None, 1)            33          fusion_dense[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 112,385,921\n",
      "Trainable params: 112,385,921\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "2426/2426 [==============================] - 83s 34ms/step - loss: 5.7579e-07 - mae: 2.9398e-04 - val_loss: 5.3541e-07 - val_mae: 2.6501e-04\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00000, saving model to best_fusion_model.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Piper\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "2426/2426 [==============================] - 74s 31ms/step - loss: 4.2696e-07 - mae: 2.5634e-04 - val_loss: 5.0945e-07 - val_mae: 2.4500e-04\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to best_fusion_model.keras\n",
      "Epoch 3/10\n",
      "2426/2426 [==============================] - 75s 31ms/step - loss: 4.2092e-07 - mae: 2.5702e-04 - val_loss: 5.4190e-07 - val_mae: 2.5097e-04\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "Epoch 4/10\n",
      "2426/2426 [==============================] - 75s 31ms/step - loss: 4.2384e-07 - mae: 2.5995e-04 - val_loss: 5.3875e-07 - val_mae: 2.6131e-04\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "Epoch 5/10\n",
      "2426/2426 [==============================] - 95s 39ms/step - loss: 4.2399e-07 - mae: 2.5658e-04 - val_loss: 5.3580e-07 - val_mae: 2.6433e-04\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "Epoch 00005: early stopping\n",
      "üíæ Â∑≤‰øùÂ≠ò‰ºòÂåñÂô®Áä∂ÊÄÅËá≥ optimizer_state_fusion.pkl\n",
      "500/607 [=======================>......] - ETA: 2s - loss: 5.2583e-07 - mae: 2.4496e-04"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Multiply, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Flatten, \n",
    "    Concatenate, Dropout, Dense  \n",
    ")\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "# ----------------------\n",
    "# Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ\n",
    "# ----------------------\n",
    "\n",
    "# 1. Áî®Êà∑ÂíåÊ≠åÊõ≤IDÁºñÁ†ÅÔºàËΩ¨Êç¢‰∏∫ËøûÁª≠Êï¥Êï∞Ôºâ\n",
    "user_encoder = LabelEncoder()\n",
    "song_encoder = LabelEncoder()\n",
    "\n",
    "# ÂØπÁî®Êà∑IDÂíåÊ≠åÊõ≤IDËøõË°åÁºñÁ†Å\n",
    "merged_data['user_id_encoded'] = user_encoder.fit_transform(merged_data['user_id'])\n",
    "merged_data['song_id_encoded'] = song_encoder.fit_transform(merged_data['song_id'])\n",
    "\n",
    "# 2. ÂΩí‰∏ÄÂåñÊí≠ÊîæÊ¨°Êï∞Âà∞ [0,1]\n",
    "max_play = merged_data['plays'].max()\n",
    "merged_data['plays_normalized'] = merged_data['plays'] / max_play\n",
    "\n",
    "# 3. ÊèêÂèñËÆ≠ÁªÉÊï∞ÊçÆ\n",
    "user_ids = merged_data['user_id_encoded'].values\n",
    "item_ids = merged_data['song_id_encoded'].values\n",
    "labels = merged_data['plays_normalized'].values  # ÂΩí‰∏ÄÂåñÂêéÁöÑÊí≠ÊîæÊ¨°Êï∞\n",
    "\n",
    "# 4. ÂàíÂàÜËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ\n",
    "train_user, test_user, train_item, test_item, train_label, test_label = train_test_split(\n",
    "    user_ids, item_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# Ê®°ÂûãÊûÑÂª∫\n",
    "# ----------------------\n",
    "\n",
    "# ÂÆö‰πâÁî®Êà∑ÂíåÁâ©ÂìÅÊï∞Èáè\n",
    "num_users = len(user_encoder.classes_)  # 105,283\n",
    "num_items = len(song_encoder.classes_)  # 384,546\n",
    "embedding_size = 32  # ÂµåÂÖ•Áª¥Â∫¶\n",
    "\n",
    "\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "def build_fusion_model(gmf_model=None, mlp_model=None):\n",
    "    pretrained_path = \"best_fusion_model.keras\"\n",
    "    \n",
    "    if Path(pretrained_path).exists():\n",
    "        print(\"‚è≥ Ê£ÄÊµãÂà∞È¢ÑËÆ≠ÁªÉFusionÊ®°ÂûãÔºåÂä†ËΩΩ‰∏≠...\")\n",
    "        model = tf.keras.models.load_model(pretrained_path)\n",
    "        print(\"‚úÖ ÊàêÂäüÂä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã\")\n",
    "        return model\n",
    "    \n",
    "    print(\"üÜï ÂàõÂª∫FusionËûçÂêàÊ®°Âûã\")\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "    def safe_load_submodel(path, prefix):\n",
    "        model = tf.keras.models.load_model(path)\n",
    "        for layer in model.layers:\n",
    "            layer._name = f\"{prefix}_{layer.name}\"\n",
    "            if isinstance(layer, tf.keras.Model):\n",
    "                for sublayer in layer.layers:\n",
    "                    sublayer._name = f\"{prefix}_{sublayer.name}\"\n",
    "        return model\n",
    "\n",
    "    # GMFÂàÜÊîØ\n",
    "    if gmf_model and Path(gmf_model).exists():\n",
    "        print(\"üí° Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉGMFÁªÑ‰ª∂\")\n",
    "        # 1. ÂÆâÂÖ®Âä†ËΩΩÂ≠êÊ®°ÂûãÂπ∂Áªü‰∏ÄÂä†ÂâçÁºÄ\n",
    "        gmf_submodel = safe_load_submodel(gmf_model, \"gmf\")\n",
    "        # 2. ÊèêÂèñÈ¢ÑËÆ≠ÁªÉÁöÑ Embedding ÊùÉÈáç\n",
    "        gmf_user_weights = gmf_submodel.get_layer(\"gmf_user_embed\").get_weights()\n",
    "        gmf_item_weights = gmf_submodel.get_layer(\"gmf_item_embed\").get_weights()\n",
    "        # 3. Áî®ÊèêÂèñÂà∞ÁöÑÊùÉÈáçÊûÑÂª∫ËûçÂêàÁî® EmbeddingÔºåÂπ∂Âä†ËΩΩÊùÉÈáç\n",
    "        fusion_gmf_user_embed = Embedding(\n",
    "            num_users, 16,\n",
    "            name=\"fusion_gmf_user_embed\",\n",
    "            \n",
    "            weights=gmf_user_weights,\n",
    "            trainable=True\n",
    "        )(user_input)\n",
    "        fusion_gmf_item_embed = Embedding(\n",
    "            num_items, 16,\n",
    "            name=\"fusion_gmf_item_embed\",\n",
    "            weights=gmf_item_weights,\n",
    "            trainable=True\n",
    "        )(item_input)\n",
    "        # 4. ÈÄêÂÖÉÁ¥†Áõ∏‰πò + ÊãâÂπ≥\n",
    "        fusion_gmf_mul     = Multiply(name=\"fusion_gmf_mul\")([fusion_gmf_user_embed, fusion_gmf_item_embed])\n",
    "        gmf_flatten        = Flatten(name=\"fusion_gmf_flatten\")(fusion_gmf_mul)\n",
    "    else:\n",
    "        print(\"üí° ÂàùÂßãÂåñÊñ∞GMFÂàÜÊîØ\")\n",
    "        gmf_user_embed = Embedding(num_users, 16, name='gmf_user_embed')(user_input)\n",
    "        gmf_item_embed = Embedding(num_items, 16, name='gmf_item_embed')(item_input)\n",
    "        gmf_output = Multiply()([Flatten()(gmf_user_embed), Flatten()(gmf_item_embed)])\n",
    "        gmf_flatten = Flatten(name='gmf_flatten')(gmf_output)\n",
    "    # MLPÂàÜÊîØ\n",
    "    if mlp_model and Path(mlp_model).exists():\n",
    "        print(\"üí° Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉMLPÁªÑ‰ª∂\")\n",
    "        # 1. ÂÆâÂÖ®Âä†ËΩΩÂ≠êÊ®°ÂûãÂπ∂Áªü‰∏ÄÂä†ÂâçÁºÄ\n",
    "        mlp_submodel = safe_load_submodel(mlp_model, \"mlp\")\n",
    "        # 2. ÊèêÂèñÈ¢ÑËÆ≠ÁªÉÁöÑ Embedding ÊùÉÈáç\n",
    "        mlp_user_weights = mlp_submodel.get_layer(\"mlp_user_embed\").get_weights()\n",
    "        mlp_item_weights = mlp_submodel.get_layer(\"mlp_item_embed\").get_weights()\n",
    "        # 3. Áî®ÊèêÂèñÂà∞ÁöÑÊùÉÈáçÊûÑÂª∫ËûçÂêàÁî® EmbeddingÔºåÂπ∂Âä†ËΩΩÊùÉÈáç\n",
    "        fusion_mlp_user_embed      = Embedding(\n",
    "            num_users, 64,\n",
    "            name=\"fusion_mlp_user_embed\",\n",
    "            weights=mlp_user_weights,\n",
    "            trainable=True\n",
    "        )(user_input)\n",
    "        fusion_mlp_item_embed      = Embedding(\n",
    "            num_items, 64,\n",
    "            name=\"fusion_mlp_item_embed\",\n",
    "            weights=mlp_item_weights,\n",
    "            trainable=True\n",
    "        )(item_input)\n",
    "        # 4. Â±ïÂπ≥ÂÜçÊãºÊé•\n",
    "        fusion_mlp_user_flatten    = Flatten(name=\"fusion_mlp_user_flatten\")(fusion_mlp_user_embed)\n",
    "        fusion_mlp_item_flatten    = Flatten(name=\"fusion_mlp_item_flatten\")(fusion_mlp_item_embed)\n",
    "        mlp_concat                 = Concatenate(name=\"fusion_mlp_concat\")(\n",
    "            [fusion_mlp_user_flatten, fusion_mlp_item_flatten]\n",
    "        )\n",
    "        # 5. Êñ∞ÁöÑ MLP ÈöêËóèÂ±Ç\n",
    "        fusion_mlp_dense1          = Dense(\n",
    "            256, activation=\"relu\", name=\"fusion_mlp_dense1\"\n",
    "        )(mlp_concat)\n",
    "        fusion_mlp_dropout         = Dropout(0.2, name=\"fusion_mlp_dropout\")(fusion_mlp_dense1)\n",
    "        fusion_mlp_dense2          = Dense(\n",
    "            128, activation=\"relu\", name=\"fusion_mlp_dense2\"\n",
    "        )(fusion_mlp_dropout)\n",
    "        mlp_output                 = Dense(\n",
    "            64, activation=\"relu\", name=\"fusion_mlp_output\"\n",
    "        )(fusion_mlp_dense2)\n",
    "    else:\n",
    "        print(\"üí° ÂàùÂßãÂåñÊñ∞MLPÂàÜÊîØ\")\n",
    "        mlp_user_embed = Embedding(num_users, 64, name='mlp_user_embed')(user_input)\n",
    "        mlp_item_embed = Embedding(num_items, 64, name='mlp_item_embed')(item_input)\n",
    "        mlp_concat = Concatenate()([Flatten()(mlp_user_embed), Flatten()(mlp_item_embed)])\n",
    "        mlp_dense = Dense(256, activation='relu')(mlp_concat)\n",
    "        mlp_dense = Dropout(0.2)(mlp_dense)\n",
    "        mlp_dense = Dense(128, activation='relu')(mlp_dense)\n",
    "        mlp_output = Dense(64, activation='relu')(mlp_dense)\n",
    "\n",
    "    # ËûçÂêàÂ±Ç\n",
    "    merged = Concatenate(name='fusion_concat')([gmf_flatten, mlp_output])\n",
    "    final_dense = Dense(32, activation='relu', name='fusion_dense')(merged)\n",
    "    output = Dense(1, activation='linear', name='fusion_output')(final_dense)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae'],\n",
    "        steps_per_execution=50,  # ÊØèÊ¨° tf.function Ë∞ÉÁî®Ë∑ë 50 ‰∏™ batch\n",
    "    )\n",
    "    return model\n",
    "# ----------------------\n",
    "# ÂàùÂßãÂåñÂπ∂ËÆ≠ÁªÉËûçÂêàÊ®°Âûã\n",
    "# ----------------------\n",
    "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁªÑ‰ª∂ÔºàÂèØÈÄâÔºâ\n",
    "model = build_fusion_model(\n",
    "    gmf_model=\"best_gmf_model.keras\",\n",
    "    mlp_model=\"best_mlp_model.keras\"\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "batch_size = 16384\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((train_user, train_item), train_label)\n",
    ").cache().shuffle(100000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((test_user, test_item), test_label)\n",
    ").cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "# Ê∑ªÂä†Ê®°Âûã‰øùÂ≠òÂõûË∞ÉÔºàËá™Âä®‰øùÂ≠òÊúÄ‰Ω≥Ê®°ÂûãÔºâ\n",
    "class CustomCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ê∑ªÂä†‰ºòÂåñÂô®Áä∂ÊÄÅ‰øùÂ≠òË∑ØÂæÑ\n",
    "        self.optimizer_path = \"optimizer_state_fusion.pkl\"\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        # ‰øùÂ≠ò‰ºòÂåñÂô®ÊùÉÈáç\n",
    "        joblib.dump(self.model.optimizer.get_weights(), self.optimizer_path)\n",
    "        print(f\"üíæ Â∑≤‰øùÂ≠ò‰ºòÂåñÂô®Áä∂ÊÄÅËá≥ {self.optimizer_path}\")\n",
    "\n",
    "# Êõ¥Êñ∞Ê£ÄÊü•ÁÇπË∑ØÂæÑ\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True   # ÊòØÂê¶ÊÅ¢Â§çÂà∞ÊúÄ‰Ω≥ÊùÉÈáç\n",
    ")\n",
    "\n",
    "checkpoint = CustomCheckpoint(\n",
    "    \"best_fusion_model.keras\",  # ‰øÆÊîπ‰øùÂ≠òÊñá‰ª∂Âêç\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Â¶ÇÊûúÊ£ÄÊµãÂà∞‰ºòÂåñÂô®Áä∂ÊÄÅÂàôÂä†ËΩΩ\n",
    "if Path(\"optimizer_state_fusion.pkl\").exists():\n",
    "    print(\"‚è≥ Âä†ËΩΩ‰ºòÂåñÂô®Áä∂ÊÄÅ...\")\n",
    "    optimizer_weights = joblib.load(\"optimizer_state_fusion.pkl\")\n",
    "    model.optimizer.set_weights(optimizer_weights)\n",
    "    print(\"‚úÖ ‰ºòÂåñÂô®Áä∂ÊÄÅÂ∑≤ÊÅ¢Â§ç\")\n",
    "\n",
    "# ËÆ≠ÁªÉÊ®°ÂûãÔºàepochsÂèØÊ†πÊçÆÈúÄË¶ÅË∞ÉÊï¥Ôºâ\n",
    "history = model.fit(\n",
    "    [train_user, train_item],             # ‰∏§‰∏™ numpy array\n",
    "    train_label,                          # Ê†áÁ≠æ\n",
    "    batch_size=16384,                     # Ê†πÊçÆÊòæÂ≠òÂèØÂ¢ûÂ§ß/ÂáèÂ∞è\n",
    "    epochs=10,\n",
    "    validation_data=([test_user, test_item], test_label),\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1                             # batch Á∫ßËøõÂ∫¶Êù°\n",
    ")\n",
    "# ----------------------\n",
    "# Ê®°ÂûãËØÑ‰º∞‰∏éÈ¢ÑÊµã\n",
    "# ----------------------\n",
    "\n",
    "# ËØÑ‰º∞ÊµãËØïÈõÜ\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f\"testsets MSE: {test_loss:.4f}, MAE: {test_mae:.4f}\")\n",
    "\n",
    "# ‰øùÂ≠òÁºñÁ†ÅÂô®ÔºàËÆ≠ÁªÉÂêéÁ´ãÂç≥ÊâßË°åÔºâ\n",
    "joblib.dump(max_play, 'max_play_fusion.pkl')\n",
    "joblib.dump(user_encoder, 'user_encoder_fusion.pkl')\n",
    "joblib.dump(song_encoder, 'song_encoder_fusion.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
